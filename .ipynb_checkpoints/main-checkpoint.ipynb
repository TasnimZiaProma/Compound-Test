{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All required python standard libraries\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All torch related imports \n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import transforms\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All sci-kit related imports \n",
    "import skimage.io as sk\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using cv2 to read an image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All sci-kit related imports \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO TRANSFORM TENSOR TO NUMPY IMAGE\n",
    "def torch_to_numpy(tensor_item):\n",
    "    return tensor_item.permute([1,2,0]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert input from BMP to grayscale and add channel information and add channel\n",
    "def custom_transform(input_image):\n",
    "    #convert to grascale  \n",
    "    input_image = rgb2gray(input_image)\n",
    "    return input_image; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_finder(predictions , labels):\n",
    "    values, max_indices = torch.max(predictions, dim=1)\n",
    "    accuracy = ( max_indices == labels ).sum()/max_indices.size()[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\Research\\\\Week 4'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_directory = os.path.join(os.getcwd(),'guides\\\\isolated-dataset-csv\\\\IsolatedTrain.csv')\n",
    "test_directory = os.path.join(os.getcwd(),'guides\\\\isolated-dataset-csv\\\\IsolatedTest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE =  2048\n",
    "#4096 \n",
    "#8192\n",
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(train_directory, usecols=[\"labels\",\"directory\"])\n",
    "test_csv = pd.read_csv(test_directory, usecols=[\"labels\",\"directory\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     labels  count\n",
      "47        1    209\n",
      "153       2    172\n",
      "101       3    199\n",
      "32        4    215\n",
      "58        5    206\n",
      "..      ...    ...\n",
      "29      167    216\n",
      "106     168    196\n",
      "28      169    217\n",
      "22      170    220\n",
      "111     171    196\n",
      "\n",
      "[171 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "res = train_csv['labels'].value_counts().to_frame().reset_index()\n",
    "res.columns = [\"labels\", \"count\"]\n",
    "res = res.sort_values(by=\"labels\",ascending=True)\n",
    "CLASS_WEIGHTS = torch.from_numpy(res.to_numpy()).float()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 171, 171, 171], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = train_csv.to_numpy()[:,0]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data loader prepraration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_NORMALIZER = transforms.Compose([transforms.ToTensor(),transforms.Resize(28),transforms.CenterCrop(28),transforms.Normalize(mean=[0.5,0.5,0.5],\n",
    "                         std=[0.5,0.5,0.5]),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.transforms.transforms.Compose"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(DATA_NORMALIZER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolated Dataset DataLoader\n",
    "\n",
    "- [ ] Dataset images should be resized to 224 * 224 * 3\n",
    "\n",
    "- [ ] Dataset labels should be one hot encoded \n",
    "\n",
    "- [ ] Apply torch transforms to images \n",
    "\n",
    "\n",
    "### Differences with Paper \n",
    "\n",
    "- [ ] Apply Dataset images should be resized to 224 * 224 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsolatedCharacterDataset(Dataset):\n",
    "    def __init__(self, csv_dir_path,  transforms=None, custom_transform=None ):\n",
    "        ### complete dataset path\n",
    "        self.dataset_csv = pd.read_csv(csv_dir_path, usecols=[\"labels\",\"directory\"])  \n",
    "        self.dataset_csv_numpy = self.dataset_csv.to_numpy()\n",
    "        \n",
    "        ### labels\n",
    "        self.labels = self.dataset_csv_numpy[:,0]\n",
    "        \n",
    "        ### images directories\n",
    "        self.image_directories = self.dataset_csv_numpy[:,1]\n",
    "        \n",
    "        ### transformations to apply on images\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # convert labels to tensor \n",
    "        label = torch.tensor(self.labels[index])\n",
    "\n",
    "        # get single image directory\n",
    "        image_dir = self.image_directories[index]\n",
    "        \n",
    "        # load single image \n",
    "        image = cv2.imread(os.path.join(os.getcwd(),image_dir), cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        if self.transforms:\n",
    "            ## apply transforms \n",
    "            image = self.transforms(image)\n",
    "            image = image.float()\n",
    "    \n",
    "        label = label.long()\n",
    "        \n",
    "        print(\"label\", label,\"index\", index)        \n",
    "        return image, label \n",
    "    \n",
    "    def __len__(self):\n",
    "        r,_ = self.dataset_csv_numpy.shape\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET = IsolatedCharacterDataset(csv_dir_path= train_directory ,transforms=DATA_NORMALIZER)\n",
    "TRAIN_LOADER = DataLoader(dataset=TRAIN_DATASET, batch_size=BATCH_SIZE ,shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (5): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=196, out_features=171, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_layers =  nn.Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "            # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            # in_channels (int) – Number of channels in the input image. For B&W it is 1.\n",
    "            # out_channels (int) – Number of channels produced by the convolution. 4 filters\n",
    "            # kernel_size (int or tuple) – Size of the convolving kernel (3x3)\n",
    "            # stride (int or tuple, optional) – Stride of the convolution\n",
    "            # padding (int or tuple, optional) – Padding of 1 added to both sides of the input\n",
    "            # example x1 = (n, c=3 , h=100 , w=100 )\n",
    "            nn.Conv2d(3, 4, kernel_size=3, stride=1, padding=1), #in_channels = 1 is a data dependent hyperparameter. It is 1 because the images are in grayscale\n",
    "            # x2 = (n, c=12 , h=100 , w=100 )\n",
    "            nn.BatchNorm2d(4), # Normalize output from the activation function. \n",
    "            nn.ReLU(inplace=True), # negative elements to zero\n",
    "            # x2 = (n, c=12 , h=100 , w=100 )\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #Stride is the number of pixels shifts over the input matrix. When the stride is 1 then we move the filters to 1 pixel at a time. When the stride is 2 then we move the filters to 2 pixels at a time and so on\n",
    "            # x3 = (n, c=12 , h=49 , w=49 )\n",
    "            # Defining another 2D convolution layer\n",
    "            nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            # x3 = (n, c=4 , h=49 , w=49 )\n",
    "            nn.BatchNorm2d(4), # 4 features\n",
    "            nn.ReLU(inplace=True), # inplace = True will modify the input directly, without allocating any additional output.\n",
    "            # x3 = (n, c=48 , h=49 , w=49 )\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # Downsamples the input representation by taking the maximum value x4 = (n, c=48 , h=25 , w=25 )\n",
    "            )\n",
    "# outputSize = floor[(inputSize - filterSize + 2 * padding) / stride] + 1\n",
    "print(cnn_layers)\n",
    "\n",
    "\n",
    "linear_layers = nn.Sequential(\n",
    "            nn.Linear(4 * 7 * 7, 171)\n",
    "# 159 classes are available for the compound classes dataset. It is a data dependent hyperparameter\n",
    ")\n",
    "\n",
    "print(linear_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.cnn_layer = cnn_layers\n",
    "        self.linear_layer = linear_layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layer(x)\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = self.linear_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available\n"
     ]
    }
   ],
   "source": [
    "optimizer =  optim.Adam(net.parameters(), lr=0.07) # learning rate \n",
    "# defining the loss function\n",
    "criterion =  nn.CrossEntropyLoss(weight=CLASS_WEIGHTS[:,1])\n",
    "if torch.cuda.is_available() and USE_CUDA:\n",
    "    print('cuda is available')\n",
    "    net = net.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(epochs:int):\n",
    "    all_training_losses = []\n",
    "    all_training_accuracy = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        total_epoch_loss = 0\n",
    "        total_accuracy_epoch = 0\n",
    "        \n",
    "        for i, data in enumerate(TRAIN_LOADER, 0): \n",
    "            image,label = data\n",
    "            optimizer.zero_grad()\n",
    "            if torch.cuda.is_available() and USE_CUDA:\n",
    "                label = label.cuda()\n",
    "                image = image.cuda()\n",
    "            output = net(image)\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_epoch_loss += loss\n",
    "            batches_training_accuracy = accuracy_finder(predictions=output, labels=label)\n",
    "            total_accuracy_epoch = total_accuracy_epoch  + batches_training_accuracy   \n",
    "        # total epoch loss \n",
    "        total_epoch_loss = total_epoch_loss / len(TRAIN_LOADER)\n",
    "        # total epoch accuracy \n",
    "        total_accuracy_epoch = total_accuracy_epoch /len(TRAIN_LOADER)\n",
    "        \n",
    "        # display the epoch training loss\n",
    "        print(\"epoch : {}/{}, loss = {:.8f}, acc = {:.8f}\".format(epoch + 1, epochs, total_epoch_loss, total_accuracy_epoch ))\n",
    "        all_training_losses.append(total_epoch_loss)\n",
    "        all_training_accuracy.append(total_accuracy_epoch)\n",
    "        \n",
    "    print(\"Training completed\")\n",
    "    return all_training_accuracy, all_training_losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2718f469c4b48609f57a72250384e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-bb05ceb4763b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mt_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-7b1cf7087b99>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(epochs)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mtotal_epoch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "t_acc, t_loss = training(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://discuss.pytorch.org/t/indexerror-target-2-is-out-of-bounds/69614/7]\n",
    "(Stack Overflow for Out of Bounds Problem)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
