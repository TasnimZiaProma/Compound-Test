{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All required python standard libraries\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All torch related imports \n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import transforms\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All sci-kit related imports \n",
    "import skimage.io as sk\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using cv2 to read an image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All sci-kit related imports \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO TRANSFORM TENSOR TO NUMPY IMAGE\n",
    "def torch_to_numpy(tensor_item):\n",
    "    return tensor_item.permute([1,2,0]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert input from BMP to grayscale and add channel information and add channel\n",
    "def custom_transform(input_image):\n",
    "    #convert to grascale  \n",
    "    input_image = rgb2gray(input_image)\n",
    "    return input_image; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_finder(predictions , labels):\n",
    "    values, max_indices = torch.max(predictions, dim=1)\n",
    "    accuracy = ( max_indices == labels ).sum()/max_indices.size()[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\Research\\\\Week 4'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_directory = os.path.join(os.getcwd(),'guides\\\\isolated-dataset-csv\\\\IsolatedTrain.csv')\n",
    "test_directory = os.path.join(os.getcwd(),'guides\\\\isolated-dataset-csv\\\\IsolatedTest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(train_directory, usecols=[\"labels\",\"directory\"])\n",
    "test_csv = pd.read_csv(test_directory, usecols=[\"labels\",\"directory\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data loader prepraration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_NORMALIZER = transforms.Compose([transforms.ToTensor(),transforms.Resize(224),transforms.CenterCrop(224),transforms.Normalize(mean=[0.5,0.5,0.5],\n",
    "                         std=[0.5,0.5,0.5]),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.transforms.transforms.Compose"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(DATA_NORMALIZER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolated Dataset DataLoader\n",
    "\n",
    "- [ ] Dataset images should be resized to 224 * 224 * 3\n",
    "\n",
    "- [ ] Dataset labels should be one hot encoded \n",
    "\n",
    "- [ ] Apply torch transforms to images \n",
    "\n",
    "\n",
    "### Differences with Paper \n",
    "\n",
    "- [ ] Apply Dataset images should be resized to 224 * 224 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsolatedCharacterDataset(Dataset):\n",
    "    def __init__(self, csv_dir_path,  transforms=None, custom_transform=None ):\n",
    "        ### complete dataset path\n",
    "        self.dataset_csv = pd.read_csv(csv_dir_path, usecols=[\"labels\",\"directory\"])  \n",
    "        self.dataset_csv_numpy = self.dataset_csv.to_numpy()\n",
    "        \n",
    "        ### labels\n",
    "        self.labels = self.dataset_csv_numpy[:,0]\n",
    "        \n",
    "        ### images directories\n",
    "        self.image_directories = self.dataset_csv_numpy[:,1]\n",
    "        \n",
    "        ### transformations to apply on images\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # convert labels to tensor \n",
    "        label = torch.tensor(self.labels[index])\n",
    "\n",
    "        # get single image directory\n",
    "        image_dir = self.image_directories[index]\n",
    "        \n",
    "        # load single image \n",
    "        image = cv2.imread(os.path.join(os.getcwd(),image_dir), cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        if self.transforms:\n",
    "            ## apply transforms \n",
    "            image = self.transforms(image)\n",
    "            image = image.float()\n",
    "    \n",
    "        label = label.long()\n",
    "         \n",
    "        return image, label \n",
    "    \n",
    "    def __len__(self):\n",
    "        r,_ = self.dataset_csv_numpy.shape\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET = IsolatedCharacterDataset(csv_dir_path= train_directory ,transforms=DATA_NORMALIZER)\n",
    "TRAIN_LOADER = DataLoader(dataset=TRAIN_DATASET, batch_size=BATCH_SIZE ,shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (5): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=196, out_features=172, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_layers =  nn.Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "            # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            # in_channels (int) – Number of channels in the input image. For B&W it is 1.\n",
    "            # out_channels (int) – Number of channels produced by the convolution. 4 filters\n",
    "            # kernel_size (int or tuple) – Size of the convolving kernel (3x3)\n",
    "            # stride (int or tuple, optional) – Stride of the convolution\n",
    "            # padding (int or tuple, optional) – Padding of 1 added to both sides of the input\n",
    "            # example x1 = (n, c=3 , h=100 , w=100 )\n",
    "            nn.Conv2d(3, 4, kernel_size=3, stride=1, padding=1), #in_channels = 1 is a data dependent hyperparameter. It is 1 because the images are in grayscale\n",
    "            # x2 = (n, c=12 , h=100 , w=100 )\n",
    "            nn.BatchNorm2d(4), # Normalize output from the activation function. \n",
    "            nn.ReLU(inplace=True), # negative elements to zero\n",
    "            # x2 = (n, c=12 , h=100 , w=100 )\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #Stride is the number of pixels shifts over the input matrix. When the stride is 1 then we move the filters to 1 pixel at a time. When the stride is 2 then we move the filters to 2 pixels at a time and so on\n",
    "            # x3 = (n, c=12 , h=49 , w=49 )\n",
    "            # Defining another 2D convolution layer\n",
    "            nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            # x3 = (n, c=4 , h=49 , w=49 )\n",
    "            nn.BatchNorm2d(4), # 4 features\n",
    "            nn.ReLU(inplace=True), # inplace = True will modify the input directly, without allocating any additional output.\n",
    "            # x3 = (n, c=48 , h=49 , w=49 )\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # Downsamples the input representation by taking the maximum value x4 = (n, c=48 , h=25 , w=25 )\n",
    "            )\n",
    "# outputSize = floor[(inputSize - filterSize + 2 * padding) / stride] + 1\n",
    "print(cnn_layers)\n",
    "\n",
    "\n",
    "linear_layers = nn.Sequential(\n",
    "            nn.Linear(4 * 7 * 7, 172)\n",
    "# 159 classes are available for the compound classes dataset. It is a data dependent hyperparameter\n",
    ")\n",
    "\n",
    "print(linear_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.cnn_layer = cnn_layers\n",
    "        self.linear_layer = linear_layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layer(x)\n",
    "\n",
    "        x = x.view(x.shape[0],-1)\n",
    "\n",
    "        x = self.linear_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available\n"
     ]
    }
   ],
   "source": [
    "optimizer =  optim.Adam(net.parameters(), lr=0.07) # learning rate \n",
    "# defining the loss function\n",
    "criterion =  nn.CrossEntropyLoss(reduction='none')\n",
    "if torch.cuda.is_available() and USE_CUDA:\n",
    "    print('cuda is available')\n",
    "    net = net.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(epochs:int):\n",
    "    all_training_losses = []\n",
    "    all_training_accuracy = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        total_epoch_loss = 0\n",
    "        total_accuracy_epoch = 0\n",
    "        \n",
    "        for i, data in enumerate(TRAIN_LOADER, 0): \n",
    "            image,label = data\n",
    "            optimizer.zero_grad()\n",
    "            if torch.cuda.is_available() and USE_CUDA:\n",
    "                label = label.cuda()\n",
    "                image = image.cuda()\n",
    "            output = net(image)\n",
    "            loss = criterion(output, label)\n",
    "            loss = loss.sum()/loss.shape[0]\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_epoch_loss += loss\n",
    "            batches_training_accuracy = accuracy_finder(predictions=output, labels=label)\n",
    "            total_accuracy_epoch = total_accuracy_epoch  + batches_training_accuracy   \n",
    "        # total epoch loss \n",
    "        total_epoch_loss = total_epoch_loss / len(TRAIN_LOADER)\n",
    "        # total epoch accuracy \n",
    "        total_accuracy_epoch = total_accuracy_epoch /len(TRAIN_LOADER)\n",
    "        \n",
    "        # display the epoch training loss\n",
    "        print(\"epoch : {}/{}, loss = {:.8f}, acc = {:.8f}\".format(epoch + 1, epochs, total_epoch_loss, total_accuracy_epoch ))\n",
    "        all_training_losses.append(total_epoch_loss)\n",
    "        all_training_accuracy.append(total_accuracy_epoch)\n",
    "        \n",
    "    print(\"Training completed\")\n",
    "    return all_training_accuracy, all_training_losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d6cd051c03458a9532a4eeac8610ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/10, loss = 6.95589685, acc = 0.00904082\n",
      "epoch : 2/10, loss = 5.13757658, acc = 0.01224706\n",
      "epoch : 3/10, loss = 5.14527845, acc = 0.00936204\n",
      "epoch : 4/10, loss = 5.13959312, acc = 0.00928330\n",
      "epoch : 5/10, loss = 5.13730860, acc = 0.00928330\n",
      "epoch : 6/10, loss = 5.13678312, acc = 0.00908647\n",
      "epoch : 7/10, loss = 5.13688517, acc = 0.00878278\n",
      "epoch : 8/10, loss = 5.13649654, acc = 0.00932267\n",
      "epoch : 9/10, loss = 5.13643551, acc = 0.00944077\n",
      "epoch : 10/10, loss = 5.13660860, acc = 0.00940140\n",
      "\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "t_acc, t_loss = training(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://discuss.pytorch.org/t/indexerror-target-2-is-out-of-bounds/69614/7]\n",
    "(Stack Overflow for Out of Bounds Problem)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
